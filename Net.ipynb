{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Net_block.ipynb\n",
    "\n",
    "\n",
    "class Net(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_arch(arch):\n",
    "        net_depth  = len(arch) - 1\n",
    "        layer_list = [None] * net_depth\n",
    "        param_list = [None] * net_depth\n",
    "        \n",
    "        input_dims = arch[0][1]\n",
    "        for i in range(1, net_depth + 1):\n",
    "            layer_list[i-1] = arch[i][0]\n",
    "            param_list[i-1] = arch[i][1]\n",
    "        return input_dims, layer_list, param_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_layers(arch):\n",
    "        input_dims, layer_list, param_list = Net.parse_arch(arch)\n",
    "        net_depth = len(layer_list)\n",
    "        layers = []\n",
    "        for i in range(net_depth):\n",
    "            layers.append(Block_mapping.module_mapping[layer_list[i]](param_list[i]))\n",
    "        return input_dims, layers\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "        \"\"\"Initialize network weights.\n",
    "        Parameters:\n",
    "            net (network)   -- network to be initialized\n",
    "            init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "            init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "        \"\"\"\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "                else:\n",
    "                    raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "            elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "                nn.init.normal_(m.weight.data, 1.0, init_gain)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "                \n",
    "        print('initialize network with %s' % init_type)\n",
    "        net.apply(init_func)  # apply the initialization function <init_func>\n",
    "    \n",
    "class Network_template(nn.Module):\n",
    "    \n",
    "    def __init__(self, ngpu, arch, attention=0):\n",
    "        super(Network_template, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.attention = attention\n",
    "        # self.main = nn.Sequential(*nn.ModuleList(arch))\n",
    "        #############################################set attention\n",
    "        if self.attention == 1 or self.attention ==5:\n",
    "            self.attention1 = nn.Sequential(*nn.ModuleList(arch[:9])) ### 6/9/12\n",
    "            self.attention2 = nn.Sequential(*nn.ModuleList(arch[9:])) ### 6/9/12\n",
    "        elif self.attention ==2 :\n",
    "            self.attention1 = nn.Sequential(*nn.ModuleList(arch[:1]))\n",
    "            self.attention2 = nn.Sequential(*nn.ModuleList(arch[1:]))\n",
    "        elif self.attention == 3:\n",
    "            self.attention1 = nn.Sequential(*nn.ModuleList(arch[:3]))\n",
    "            self.attention2 = nn.Sequential(*nn.ModuleList(arch[3:7]))\n",
    "            self.attention3 = nn.Sequential(*nn.ModuleList(arch[7:11]))\n",
    "            self.attention4 = nn.Sequential(*nn.ModuleList(arch[11:15]))\n",
    "            self.attention_remain = nn.Sequential(*nn.ModuleList(arch[15:]))\n",
    "        elif self.attention == 4:\n",
    "            self.attention1 = nn.Sequential(*nn.ModuleList(arch[:9]))\n",
    "            self.attention2 = nn.Sequential(*nn.ModuleList(arch[9:]))\n",
    "        else:\n",
    "            self.main = nn.Sequential(*nn.ModuleList(arch))\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            # print('test...............',self.training)\n",
    "            if self.attention == 1 or self.attention ==5:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output1,mask = nn.parallel.data_parallel(self.attention1, input, range(self.ngpu))\n",
    "                    output2 = nn.parallel.data_parallel(self.attention2, output1, range(self.ngpu))\n",
    "                else:\n",
    "                    output1,mask = self.attention1(input)\n",
    "                    output2      = self.attention2(output1)\n",
    "                return output2,mask\n",
    "            \n",
    "            elif self.attention == 2:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output1,mask = nn.parallel.data_parallel(self.attention1, input, range(self.ngpu))\n",
    "                    output2 = nn.parallel.data_parallel(self.attention2, output1, range(self.ngpu))\n",
    "                else:\n",
    "                    output1,mask = self.attention1(input)\n",
    "                    output2      = self.attention2(output1)\n",
    "                return output2,mask\n",
    "            \n",
    "            elif self.attention == 3:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output1,mask1 = nn.parallel.data_parallel(self.attention1, input, range(self.ngpu))\n",
    "                    output2,mask2 = nn.parallel.data_parallel(self.attention2, output1, range(self.ngpu))\n",
    "                    output3,mask3 = nn.parallel.data_parallel(self.attention3, output2, range(self.ngpu))\n",
    "                    output4,mask4 = nn.parallel.data_parallel(self.attention4, output3, range(self.ngpu))\n",
    "                    output = nn.parallel.data_parallel(self.attention_remain, output4, range(self.ngpu))\n",
    "                else:\n",
    "                    output1,mask1 = self.attention1(input)\n",
    "                    output2,mask2 = self.attention2(output1)\n",
    "                    output3,mask3 = self.attention3(output2)\n",
    "                    output4,mask4 = self.attention4(output3)\n",
    "                    output = self.attention_remain(output4)\n",
    "                return output,mask1,mask2,mask3,mask4\n",
    "            \n",
    "            elif self.attention == 4:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output1,mask = nn.parallel.data_parallel(self.attention1, input, range(self.ngpu))\n",
    "                    output2 = nn.parallel.data_parallel(self.attention2, output1, range(self.ngpu))\n",
    "                else:\n",
    "                    output1,mask = self.attention1(input)\n",
    "                    output2      = self.attention2(output1)\n",
    "                return output2,mask\n",
    "\n",
    "            else:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "                else:\n",
    "                    output = self.main(input)\n",
    "                #return torch.squeeze(output)\n",
    "                return output\n",
    "            \n",
    "        else:\n",
    "            # print('test...............',self.training)\n",
    "            if self.attention == 1 or self.attention == 5:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output1 = nn.parallel.data_parallel(self.attention1, input, range(self.ngpu))\n",
    "                    output2 = nn.parallel.data_parallel(self.attention2, output1, range(self.ngpu))\n",
    "                else:\n",
    "                    output1 = self.attention1(input)\n",
    "                    output2 = self.attention2(output1)\n",
    "                return output2\n",
    "            \n",
    "            elif self.attention == 2:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output1  = nn.parallel.data_parallel(self.attention1, input, range(self.ngpu))\n",
    "                    output2 = nn.parallel.data_parallel(self.attention2, output1, range(self.ngpu))\n",
    "                else:\n",
    "                    output1 = self.attention1(input)\n",
    "                    output2 = self.attention2(output1)\n",
    "                return output2\n",
    "            \n",
    "            elif self.attention == 3:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output1 = nn.parallel.data_parallel(self.attention1, input, range(self.ngpu))\n",
    "                    output2 = nn.parallel.data_parallel(self.attention2, output1, range(self.ngpu))\n",
    "                    output3 = nn.parallel.data_parallel(self.attention3, output2, range(self.ngpu))\n",
    "                    output4 = nn.parallel.data_parallel(self.attention4, output3, range(self.ngpu))\n",
    "                    output = nn.parallel.data_parallel(self.attention_remain, output4, range(self.ngpu))\n",
    "                else:\n",
    "                    output1 = self.attention1(input)\n",
    "                    output2 = self.attention2(output1)\n",
    "                    output3 = self.attention3(output2)\n",
    "                    output4 = self.attention4(output3)\n",
    "                    output = self.attention_remain(output4)\n",
    "                return output\n",
    "            \n",
    "            elif self.attention == 4:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output1 = nn.parallel.data_parallel(self.attention1, input, range(self.ngpu))\n",
    "                    output2 = nn.parallel.data_parallel(self.attention2, output1, range(self.ngpu))\n",
    "                else:\n",
    "                    output1 = self.attention1(input)\n",
    "                    output2 = self.attention2(output1)\n",
    "                return output2\n",
    "\n",
    "            else:\n",
    "                if input.is_cuda and self.ngpu > 1:\n",
    "                    output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "                else:\n",
    "                    output = self.main(input)\n",
    "                #return torch.squeeze(output)\n",
    "                return output\n",
    "\n",
    "#########################################zzl\n",
    "# class BCEFocalLoss(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     二分类的Focalloss alpha 固定\n",
    "#     alpha is the weight for loss p:0 -> l:1 for us: alpha > 0.5 because 0 for most\n",
    "#     \"\"\"\n",
    "#     def __init__(self, gamma, alpha, reduction='mean'):\n",
    "#         super().__init__()\n",
    "#         self.gamma = gamma\n",
    "#         self.alpha = alpha\n",
    "#         self.reduction = reduction\n",
    " \n",
    "#     def forward(self, _input, target):\n",
    "#         pt = torch.sigmoid(_input)\n",
    "#         alpha = self.alpha\n",
    "#         loss = - alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
    "#                (1 - alpha) * pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "#         if self.reduction == 'mean':\n",
    "#             loss = torch.mean(loss)\n",
    "#         elif self.reduction == 'sum':\n",
    "#             loss = torch.sum(loss)\n",
    "#         return loss\n",
    "    \n",
    "class BCEFocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma,alpha,reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha=alpha\n",
    "        self.reduction = reduction\n",
    "    def forward(self, input, target):\n",
    "        # input:size is M*2. M　is the batch　number\n",
    "        # target:size is M.\n",
    "        pt=torch.softmax(input,dim=1)\n",
    "        p=pt[:,1]\n",
    "        loss = -self.alpha*(1-p)**self.gamma*(target*torch.log(p))-\\\n",
    "               (1-self.alpha)*p**self.gamma*((1-target)*torch.log(1-p))\n",
    "        if self.reduction == 'mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "class StandardLoss(nn.Module):\n",
    "    def __init__(self, mode, reduction, gamma, alpha, theta=0.0):\n",
    "        super(StandardLoss, self).__init__()\n",
    "        self.loss_mode = mode\n",
    "        self.theta = theta\n",
    "        if mode == 'xentropy':\n",
    "            self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "        elif mode == 'focal':\n",
    "            self.criterion = BCEFocalLoss(gamma= gamma, alpha= alpha, reduction=reduction)\n",
    "        else:\n",
    "            raise NotImplementedError('loss mode %s not implemented' % mode)\n",
    "            \n",
    "    def xentropy_loss(self, params):\n",
    "        # params: [0]net, [1]Dtrain, [2]labels\n",
    "        assert(len(params) == 3)\n",
    "        predictions = params[0](params[1])\n",
    "        loss = self.criterion(predictions, params[2])\n",
    "        return loss\n",
    "\n",
    "    def attention_loss(self,params):\n",
    "        # params: [0]net, [1]Dtrain, [2]labels, [3]pred_mask1, [4]pred_mask2, [5]topo_mask1, [6]topo_mask2\n",
    "        assert(len(params) == 7)\n",
    "        predictions = params[0](params[1])\n",
    "\n",
    "        size1 = params[3].size()\n",
    "        size2 = params[4].size()\n",
    "        \n",
    "        params[5] = params[5].resize_(size1)\n",
    "        params[6] = params[6].resize_(size2)\n",
    "\n",
    "        # factor1 = params[3].size(1)/params[5].size(1)\n",
    "        # factor2 = params[4].size(1)/params[6].size(1)\n",
    "        # params[5] = torch.nn.functional.interpolate(params[5], scale_factor=factor1, mode='bilinear',\n",
    "        #                                             align_corners=False)\n",
    "        # params[6] = torch.nn.functional.interpolate(params[6], scale_factor=factor2, mode='bilinear',\n",
    "        #                                             align_corners=False)\n",
    "\n",
    "        l1 = F.mse_loss(params[3],params[5])\n",
    "        l2 = F.mse_loss(params[4],params[6])\n",
    "\n",
    "        att_loss = l1+l2\n",
    "        class_loss = self.criterion(predictions, params[2])\n",
    "        loss = class_loss + self.theta*att_loss\n",
    "        return loss\n",
    "    \n",
    "    def attention_loss_onestream(self,params):\n",
    "        # params: [0]net, [1]Dtrain, [2]labels, [3]pred_mask, [4]topo_mask\n",
    "        assert(len(params) == 5)\n",
    "        predictions = params[0](params[1])\n",
    "\n",
    "        size = params[3].size()\n",
    "        \n",
    "        params[4] = params[4].resize_(size)\n",
    "\n",
    "        att_loss = F.mse_loss(params[3],params[4])\n",
    "\n",
    "        class_loss = self.criterion(predictions, params[2])\n",
    "        loss = class_loss + self.theta*att_loss\n",
    "        return loss\n",
    "    \n",
    "    def attention_loss_multi(self,params):\n",
    "        # params: [0]net, [1]Dtrain, [2]labels, [3]pred_mask11, [4]pred_mask12, [5]pred_mask13, [6]pred_mask14, [7]pred_mask21,[8]pred_mask22,[9]pred_mask23,[10]pred_mask24,[11]topo_mask1, [12]topo_mask2\n",
    "        assert(len(params) == 13)\n",
    "        predictions = params[0](params[1])\n",
    "\n",
    "        size1 = params[3].size()\n",
    "        size2 = params[4].size()\n",
    "        size3 = params[5].size()\n",
    "        size4 = params[6].size()\n",
    "       \n",
    "        \n",
    "        mask11_label = params[11].resize_(size1)\n",
    "        # mask11_label = nn.functional.interpolate(params[11], size=tuple(size1[-3:]), mode='trilinear', align_corners=True)\n",
    "        l11 = nn.SmoothL1Loss(reduction='mean')(params[3],mask11_label)\n",
    "        mask12_label = params[11].resize_(size2)\n",
    "        # mask12_label = nn.functional.interpolate(params[11], size=tuple(size2[-3:]), mode='trilinear', align_corners=True)\n",
    "        l12 = nn.SmoothL1Loss(reduction='mean')(params[4],mask12_label)\n",
    "        mask13_label = params[11].resize_(size3)\n",
    "        # mask13_label = nn.functional.interpolate(params[11], size=tuple(size3[-3:]), mode='trilinear', align_corners=True)\n",
    "        l13 = nn.SmoothL1Loss(reduction='mean')(params[5],mask13_label)\n",
    "        mask14_label = params[11].resize_(size4)\n",
    "        # mask14_label = nn.functional.interpolate(params[11], size=tuple(size4[-3:]), mode='trilinear', align_corners=True)\n",
    "        l14 = nn.SmoothL1Loss(reduction='mean')(params[6],mask14_label)\n",
    "\n",
    "        mask21_label = params[12].resize_(size1)\n",
    "        # mask21_label = nn.functional.interpolate(params[12], size=tuple(size1[-3:]), mode='trilinear', align_corners=True)\n",
    "        l21 = nn.SmoothL1Loss(reduction='mean')(params[7],mask21_label)\n",
    "        mask22_label = params[12].resize_(size2)\n",
    "        # mask22_label = nn.functional.interpolate(params[12], size=tuple(size2[-3:]), mode='trilinear', align_corners=True)\n",
    "        l22 = nn.SmoothL1Loss(reduction='mean')(params[8],mask22_label)\n",
    "        mask23_label = params[12].resize_(size3)\n",
    "        # mask23_label = nn.functional.interpolate(params[12], size=tuple(size3[-3:]), mode='trilinear', align_corners=True)\n",
    "        l23 = nn.SmoothL1Loss(reduction='mean')(params[9],mask23_label)\n",
    "        mask24_label = params[12].resize_(size4)\n",
    "        # mask24_label = nn.functional.interpolate(params[12], size=tuple(size4[-3:]), mode='trilinear', align_corners=True)\n",
    "        l24 = nn.SmoothL1Loss(reduction='mean')(params[10],mask24_label)\n",
    "\n",
    "        # print('mse1:',l11.item(),l21.item())\n",
    "        # print('mse2:',l12.item(),l22.item())\n",
    "        # print('mse3:',l13.item(),l23.item())\n",
    "        # print('mse4:',l14.item(),l24.item())\n",
    "\n",
    "        att_loss = l11+l12+l13+l14+l21+l22+l23+l24\n",
    "        class_loss = self.criterion(predictions, params[2])\n",
    "        loss = class_loss + self.theta*att_loss\n",
    "        # #################################### print test\n",
    "        # print(l11,l12,l13,l14)\n",
    "        # print(l21,l22,l23,l24)\n",
    "        # print(class_loss)\n",
    "        # ####################################\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, params):\n",
    "        if self.loss_mode == 'xentropy' or 'focal':\n",
    "            if self.theta == 0.0:\n",
    "                assert(len(params)==3)\n",
    "                loss = self.xentropy_loss(params)\n",
    "            else:\n",
    "                if len(params) == 7:\n",
    "                    # assert(len(params)==7)\n",
    "                    loss = self.attention_loss(params)\n",
    "                elif len(params) == 5:\n",
    "                    loss = self.attention_loss_onestream(params)\n",
    "                else:\n",
    "                    assert(len(params)==13)\n",
    "                    loss = self.attention_loss_multi(params) \n",
    "        return loss\n",
    "#########################################zzl\n",
    "\n",
    "# class StandardLoss(nn.Module):\n",
    "    \n",
    "#     def __init__(self, mode, reduction, gamma, alpha):\n",
    "#         super(StandardLoss, self).__init__()\n",
    "#         self.loss_mode = mode\n",
    "#         if mode == 'xentropy':\n",
    "#             self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "#         elif mode == 'focal':\n",
    "#             self.criterion = BCEFocalLoss(gamma= gamma, alpha= alpha, reduction=reduction)\n",
    "#         else:\n",
    "#             raise NotImplementedError('loss mode %s not implemented' % mode)\n",
    "            \n",
    "#     def xentropy_loss(self, params):\n",
    "#         # params: [0]net, [1]Dtrain, [2]labels\n",
    "#         assert(len(params) == 3)\n",
    "#         predictions = params[0](params[1])\n",
    "#         loss = self.criterion(predictions, params[2])\n",
    "#         return loss\n",
    "    \n",
    "#     def __call__(self, params):\n",
    "#         if self.loss_mode == 'xentropy' or 'focal':\n",
    "#             loss = self.xentropy_loss(params)\n",
    "#         return loss\n",
    "    \n",
    "class GANLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, gan_mode, reduction):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(1.0))\n",
    "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.criterion = nn.MSELoss(reduction=reduction)\n",
    "        elif gan_mode == 'vanilla' or gan_mode == \"vanilla_topo\":\n",
    "            self.criterion = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        elif gan_mode == \"wgangp\" or gan_mode == \"wgan\":\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "            \n",
    "    def get_target_tensor(self, prediction, is_real):\n",
    "        if is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "    \n",
    "    def calc_gradient_penalty(self, netD, device, Dreal, Dfake, constant=1.0, lambda_gp=10.0):\n",
    "        batch_size = Dreal.shape[0]\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "        alpha = alpha.expand_as(Dreal)\n",
    "        interpolated = alpha * Dreal + (1 - alpha) * Dfake\n",
    "        interpolated.requires_grad_(True)\n",
    "        out_interp = netD(interpolated)\n",
    "        gradients = torch.autograd.grad(outputs=out_interp, inputs=interpolated,\n",
    "                    grad_outputs=torch.ones(out_interp.size()).to(device),\n",
    "                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        gradients  = gradients.view(batch_size, -1)\n",
    "        grad_norm  = torch.sqrt(torch.sum(gradients**2, dim=1) + 1e-12)\n",
    "        gp_penalty = ((grad_norm - constant)**2).mean()\n",
    "        return gp_penalty * lambda_gp\n",
    "    \n",
    "    def vanilla_topo_loss(self, params):\n",
    "        '''\n",
    "        params: Dfake_device, Dfix_device\n",
    "        both Dfake_device and Dfix_device should be output from tanh() layer,\n",
    "        which have values between -1.0 and 1.0.\n",
    "        '''\n",
    "        image_shape = list(params[0].shape)\n",
    "        image_shape = image_shape[-2:]\n",
    "        flat_fake = params[0].view(-1, np.prod(image_shape))\n",
    "        flat_fix  = params[1].view(-1, np.prod(image_shape))\n",
    "        topo_err  = self.criterion(flat_fake, flat_fix)\n",
    "        return topo_err\n",
    "    \n",
    "    def wgan_gp_loss(self, params):\n",
    "        if params[0] == \"G\":\n",
    "            # if G params[1]: netD, params[2]: Dfake\n",
    "            assert(len(params) == 3)\n",
    "            out_fake = params[1](params[2])\n",
    "            err_fake = -out_fake.mean()\n",
    "            return err_fake\n",
    "        elif params[0] == \"D\":\n",
    "            # if D params[1]: netD, params[2]: device, params[3]: Dreal, params[4]: Dfake\n",
    "            assert(len(params) >= 5)\n",
    "            out_real = params[1](params[3])\n",
    "            out_fake = params[1](params[4].detach())\n",
    "            gp_penalty = self.calc_gradient_penalty(params[1], params[2], params[3], params[4].detach())\n",
    "            return out_fake.mean() - out_real.mean() + gp_penalty\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized network' % params[0])\n",
    "            \n",
    "    def wgan_loss(self, params):\n",
    "        if params[0] == \"G\":\n",
    "            # if G params[1]: netD, params[2]: Dfake\n",
    "            assert(len(params) == 3)\n",
    "            out_fake = params[1](params[2])\n",
    "            err_fake = -out_fake.mean()\n",
    "            return err_fake\n",
    "        elif params[0] == \"D\":\n",
    "            # if D params[1]: netD, params[2]: device, params[3]: Dreal, params[4]: Dfake\n",
    "            assert(len(params) >= 5)\n",
    "            out_real = params[1](params[3])\n",
    "            out_fake = params[1](params[4].detach())\n",
    "            return out_fake.mean() - out_real.mean()\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized network' % params[0])\n",
    "            \n",
    "    def gan_loss(self, params):\n",
    "        if params[0] == \"G\":\n",
    "            # if G params[1]: netD, params[2]: Dfake\n",
    "            assert(len(params) == 3)\n",
    "            out_fake      = params[1](params[2])\n",
    "            target_tensor = self.get_target_tensor(out_fake, True)\n",
    "            err_fake      = self.criterion(out_fake, target_tensor)\n",
    "            return err_fake\n",
    "        elif params[0] == \"D\":\n",
    "            # if D params[1]: netD, params[2]: device, params[3]: Dreal, params[4]: Dfake\n",
    "            assert(len(params) >= 5)\n",
    "            out_real      = params[1](params[3])\n",
    "            out_fake      = params[1](params[4].detach())\n",
    "            target_tensor = self.get_target_tensor(out_real, True)\n",
    "            err_real      = self.criterion(out_real, target_tensor)\n",
    "            target_tensor = self.get_target_tensor(out_fake, False)\n",
    "            err_fake      = self.criterion(out_fake, target_tensor)\n",
    "            return err_real + err_fake\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized network' % params[0])\n",
    "        \n",
    "    def __call__(self, params):\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            loss = self.gan_loss(params)\n",
    "        elif self.gan_mode == 'vanilla_topo':\n",
    "            loss = self.vanilla_topo_loss(params)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            loss = self.wgan_gp_loss(params)\n",
    "        elif self.gan_mode == \"wgan\":\n",
    "            loss = self.wgan_loss(params)\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topo2",
   "language": "python",
   "name": "topo2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a587393fa550700e0fba79d0bdedf45b42916a28255a920c0c28087d13312d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
