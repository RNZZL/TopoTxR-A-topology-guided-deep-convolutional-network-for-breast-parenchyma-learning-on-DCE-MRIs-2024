{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Include.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run Medical_IO.ipynb\n",
    "%run Argparser.ipynb\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import cv2\n",
    "from scipy import signal\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from monai.data import ImageDataset\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from monai.transforms import AddChannel, Compose, ScaleIntensity, RandAffine, EnsureType,Rand3DElastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_settings_extend():\n",
    "    \n",
    "    settings = dict()\n",
    "    settings[\"Basic\"] = {\n",
    "        \"branch_name\":       \"extend\",         #\"map_patch64\", cremi_patch64\", \"isbi_patch64\", \"retina_patch128\"\n",
    "        \"continue_model\":    False,\n",
    "        \"model_step\":        0,\n",
    "        \"data_extension\":    \"nii\",\n",
    "        \"dataset\":           \"nii\"\n",
    "    }\n",
    "    settings[\"Path\"] = {\n",
    "        \"save_path0\":         \"/home/zhilin/TopoTxR_backup/data/extend_ori2\",\n",
    "        \"save_path1\":         \"/home/zhilin/TopoTxR_backup/data/extend_dim1_\",\n",
    "        \"save_path2\":         \"/home/zhilin/TopoTxR_backup/data/extend_dim2_\",\n",
    "        'ori_path':          \"/home/zhilin/TopoTxR_backup/data/data_ori\",\n",
    "        \"data_path\":         \"/home/zhilin/TopoTxR_backup/data/whole_256_th2_dim1_dil0_fv\",\n",
    "        \"data_path2\":        \"/home/zhilin/TopoTxR_backup/data/whole_256_th2_dim2_dil0_fv\"\n",
    "    }\n",
    "    # settings[\"Monitor\"] = {\n",
    "    #     \"print_step\":        20,\n",
    "    #     \"save_step\":         3000\n",
    "    # }\n",
    "    # settings[\"GPU\"] = {\n",
    "    #     \"gpu_num\":           1,\n",
    "    #     \"gpu_enable\":        True,\n",
    "    #     \"cudnn_benchmark\":   True\n",
    "    # }\n",
    "    return settings\n",
    "\n",
    "\n",
    "def return_data_settings_extend():\n",
    "    data_settings = dict()\n",
    "    data_settings = {\n",
    "        \"epochs\":           500,\n",
    "        \"batch_size\":       1,\n",
    "        \"batch_workers\":    0,\n",
    "        \"shuffle\":          True,\n",
    "        \"drop_last\":        False,\n",
    "        'transform':        True,\n",
    "        \"datasplit_scheme\": \"Test\",        # options: All|Test|Valid\n",
    "        \"test_split\":       0.2,           # obsolete parameter\n",
    "        \"xfold\":            10,\n",
    "        \"fold_idx\":         0,\n",
    "        \"random_seed\":      64\n",
    "    }\n",
    "    return data_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings    = return_settings_extend()\n",
    "data_params = return_data_settings_extend()\n",
    "print(settings)\n",
    "print(data_params)\n",
    "\n",
    "epochs           = data_params[\"epochs\"]\n",
    "batch_size       = data_params[\"batch_size\"]\n",
    "batch_workers    = data_params[\"batch_workers\"]\n",
    "shuffle          = data_params[\"shuffle\"]\n",
    "drop_last        = data_params[\"drop_last\"]\n",
    "transform        = data_params['transform']\n",
    "datasplit_scheme = data_params[\"datasplit_scheme\"]\n",
    "test_split       = data_params[\"test_split\"]\n",
    "xfold            = data_params[\"xfold\"]\n",
    "fold_idx         = data_params[\"fold_idx\"]\n",
    "random_seed      = data_params[\"random_seed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileIO_MEDICAL(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_nii(pathIn):\n",
    "        struct = nib.load(pathIn)\n",
    "        #print(\"Data type is: \", struct.get_data_dtype())\n",
    "        return struct.get_fdata()\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_nii(data, pathOut):\n",
    "        struct = nib.Nifti1Image(data, np.eye(4))\n",
    "        nib.save(struct, pathOut)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataset_wValidation_extend(name, data_path, batch_size, batch_workers, shuffle, drop_last, scalor, datasplit_scheme, test_split, xfold, fold_idx, random_seed=-1):\n",
    "\n",
    "    set_global_random_seed(random_seed)\n",
    "\n",
    "    if datasplit_scheme==\"Test\":\n",
    "        print(\"Test mode in data_aug fetcher.\")\n",
    "        os.chdir(data_path)\n",
    "        data_address = []\n",
    "        data_labels = []\n",
    "        for file in glob.glob(\"*.\"+ name ):\n",
    "            data_address.append(os.path.join(data_path, file))\n",
    "            data_labels.append(int(file.split('_')[2]))\n",
    "        \n",
    "        if shuffle:\n",
    "            state = np.random.get_state()\n",
    "            np.random.shuffle(data_address)\n",
    "            np.random.set_state(state)\n",
    "            np.random.shuffle(data_labels)\n",
    "            np.random.set_state(state)\n",
    "\n",
    "       \n",
    "        # sample_num = len(data_address)\n",
    "        # fold_size = sample_num // xfold\n",
    "\n",
    "        ##define transfomation:\n",
    "        train_transform = Compose([ScaleIntensity(), AddChannel(), EnsureType(), Rand3DElastic(sigma_range=(5, 8),magnitude_range=(100, 200),prob= 1, padding_mode='zeros',rotate_range=(np.pi , np.pi , np.pi),shear_range=(np.pi , np.pi , np.pi),translate_range=(32, 32, 32),scale_range=(0.15, 0.15, 0.15))])\n",
    "        # train_transform = Compose([ScaleIntensity(), AddChannel(), EnsureType(), RandAffine(prob= 1, padding_mode='border',rotate_range=(np.pi , np.pi , np.pi),shear_range=(np.pi , np.pi , np.pi),translate_range=(32, 32, 32),scale_range=(0.15, 0.15, 0.15))])\n",
    "        # train_transform = Compose([ScaleIntensity(), AddChannel(), EnsureType()])\n",
    "        # val_transform = Compose([ScaleIntensity(), AddChannel(), EnsureType()])\n",
    "\n",
    "\n",
    "        # if fold_idx < xfold -1:\n",
    "        #     val_start = fold_idx * fold_size\n",
    "        #     val_end = (fold_idx+1) * fold_size\n",
    "        #     address_val, label_val = data_address[val_start:val_end], data_labels[val_start:val_end]\n",
    "        #     address_train = data_address[0:val_start]\n",
    "        #     address_train.extend(data_address[val_end:])\n",
    "        #     # label_train = np.concatenate(data_labels[:val_start], data_labels[val_end:], axis =0)\n",
    "        #     label_train = data_labels[:val_start]\n",
    "        #     label_train.extend(data_labels[val_end:])\n",
    "        \n",
    "        # else:\n",
    "        #     val_start = fold_idx * fold_size\n",
    "        #     address_val, label_val = data_address[val_start:], data_labels[val_start:]\n",
    "        #     address_train, label_train = data_address[:val_start], data_labels[:val_start]\n",
    "        \n",
    "        label_train = np.array(data_labels, dtype=np.int64)\n",
    "        # label_val   = np.array(label_val, dtype=np.int64)\n",
    "\n",
    "        address_train = data_address\n",
    "\n",
    "        train_ds = ImageDataset(image_files=address_train,labels = label_train,transform=train_transform)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "                                pin_memory=torch.cuda.is_available())\n",
    "        # val_ds = ImageDataset(image_files=address_val,labels = label_val,transform=val_transform)\n",
    "        # val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "        #                     pin_memory=torch.cuda.is_available())\n",
    "        \n",
    "        return train_loader,address_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader0, address_train0 = fetch_dataset_wValidation_extend(settings['Basic']['dataset'], settings['Path']['ori_path'], batch_size, batch_workers, shuffle, drop_last, 0.5, datasplit_scheme, test_split, xfold, fold_idx, random_seed)\n",
    "train_loader1, address_train1 = fetch_dataset_wValidation_extend(settings['Basic']['dataset'], settings['Path']['data_path'], batch_size, batch_workers, shuffle, drop_last, 0.5, datasplit_scheme, test_split, xfold, fold_idx, random_seed)\n",
    "train_loader2, address_train2 = fetch_dataset_wValidation_extend(settings['Basic']['dataset'], settings['Path']['data_path2'], batch_size, batch_workers, shuffle, drop_last, 0.5, datasplit_scheme, test_split, xfold, fold_idx, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = address_train0[0]\n",
    "# print(a.split('/')[-1].split('.')[0])\n",
    "# data_address.append(os.path.join(data_path, file))\n",
    "\n",
    "\n",
    "def write_address_Out(address_train,out_folder):\n",
    "    address_out = []\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.makedirs(out_folder)\n",
    "    for i,address in enumerate(address_train):\n",
    "        file_name = (address.split('/')[-1]).split('.')[0] +'_aug.nii'\n",
    "        out_path = os.path.join(out_folder,file_name)\n",
    "        address_out.append(out_path)\n",
    "    return address_out\n",
    "\n",
    "address_out0 = write_address_Out(address_train0,settings['Path']['save_path0'])\n",
    "address_out1 = write_address_Out(address_train1,settings['Path']['save_path1'])\n",
    "address_out2 = write_address_Out(address_train2,settings['Path']['save_path2'])\n",
    "\n",
    "print(address_out0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_dataset(train_loader,address_out):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        vol = ((data[0]).squeeze(0)).squeeze(0).numpy()\n",
    "        label = data[1].numpy()\n",
    "        if label == 0:\n",
    "            continue\n",
    "        else:\n",
    "            FileIO_MEDICAL.save_nii(vol,address_out[i])\n",
    "    return 0\n",
    "        \n",
    "#_ = extend_dataset(train_loader0,address_out0)\n",
    "_ = extend_dataset(train_loader1,address_out1)   \n",
    "_ = extend_dataset(train_loader2,address_out2)                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a587393fa550700e0fba79d0bdedf45b42916a28255a920c0c28087d13312d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
