{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Include.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run Medical_IO.ipynb\n",
    "%run Archpool.ipynb\n",
    "%run Argparser.ipynb\n",
    "\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "from scipy import signal\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from monai.transforms import LoadImage, Randomizable, apply_transform, AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, RandAffine, RandGaussianNoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NII_with_label(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, target_type, random_seed, transform=None, transforms_monai = None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.transform_monai = transforms_monai\n",
    "        self.address_book = []\n",
    "        self.labels = []\n",
    "        self._seed =  random_seed\n",
    "        self.kernel = Util_gen.generate_gaussian_kernel(3, 2, 3)\n",
    "        \n",
    "        os.chdir(root)\n",
    "        for file in glob.glob(\"*.\"+target_type):\n",
    "            self.address_book.append(os.path.join(root, file))\n",
    "            self.labels.append(int(file.split('_')[2]))\n",
    "        if (target_type == 'nii'):\n",
    "            tease = FileIO_MEDICAL.load_nii(self.address_book[0])\n",
    "        else:\n",
    "            print(\"Data_with_label: unrecognized data type\")\n",
    "        print(\"Image shape: \" + str(tease.shape))\n",
    "        print(\"Image value range: %.2f - %.2f\" %(np.amin(tease), np.amax(tease)))\n",
    "        print(\"Image data type\" + str(type(tease[0][0][0])))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.address_book)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        print('idx is here:', idx)\n",
    "        vol = np.float32(FileIO_MEDICAL.load_nii(self.address_book[idx]))\n",
    "        #vol = signal.convolve(vol, self.kernel, mode='same')\n",
    "        if self.transform and self.transform_monai :\n",
    "            self.transform_monai.set_random_state(seed=self._seed)\n",
    "            vol = self.transform(vol)\n",
    "            vol = apply_transform(self.transform_monai, vol, map_items=False)\n",
    "        instance = {'vol': vol, 'label': self.labels[idx]}\n",
    "        return instance\n",
    "\n",
    "\n",
    "class Data_fetcher(object):\n",
    "    @staticmethod\n",
    "    def fetch_dataset_wValidation(name, data_path, batch_size, batch_workers, shuffle, drop_last, scalor, datasplit_scheme, test_split, xfold, fold_idx, random_seed=-1):\n",
    "        '''\n",
    "        This is the advanced version to fetch_dataset with validation split, it works like this:\n",
    "        the data is split into [train+validation][test] according to test_split\n",
    "        the [train+validation] is further split into [train][validation] according to valid_split\n",
    "        @name: name of the dataset\n",
    "        @batch_size: number of instances per batch\n",
    "        @batch_workers: number of workers to fetch data\n",
    "        @shuffle: if to shuffle the data\n",
    "        @drop_last: drop the instances that do not fit in the last batch\n",
    "        @scalor: scale the data\n",
    "        @datasplit_scheme: \"All\" use all data for training; \"Test\" partitions into train/test according to test_split; \"Valid\" paritions into train/test/validation\n",
    "        @test_split: percentage of the data for test\n",
    "        @xfold: number of folds for validation\n",
    "        @fold_idx: x-fold cross validation, indicates which fold to use as validation\n",
    "        @random_seed: for random indices shuffle purpose\n",
    "        '''\n",
    "        if name == \"cifar10\":\n",
    "            dataset = dset.CIFAR10(root=data_path, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize([64, 64]),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                      ]))\n",
    "        elif name == \"celeba\": # The data should be under a folder under root: root/celeba/*.png\n",
    "            dataset = dset.ImageFolder(root=data_path,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(64),\n",
    "                          transforms.CenterCrop(64),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                      ]))\n",
    "        elif name == \"topo\":\n",
    "            dataset = Data_topo(data_path, FLAGS.pds_path, FLAGS.data_extension,\n",
    "                  transform=transforms.Compose(\n",
    "                 [transforms.ToPILImage(),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([scalor], [scalor])\n",
    "                 ]))\n",
    "        elif name == \"dmt\":\n",
    "            dataset = Data_dmt(data_path, FLAGS.data_extension,\n",
    "                  transform=transforms.Compose(\n",
    "                 [transforms.ToPILImage(),\n",
    "                  transforms.ToTensor()\n",
    "                 ]))\n",
    "        elif name == \"nii\":\n",
    "            dataset = NII_with_label(data_path, FLAGS.data_extension, random_seed,\n",
    "                  transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                 transforms_monai=Compose(\n",
    "                [RandAffine(prob= 0.3,rotate_range=(np.pi / 4, np.pi / 4, np.pi / 4),translate_range=(32, 32, 32),scale_range=(0.15, 0.15, 0.15)), RandGaussianNoise(prob=1.0)\n",
    "                 ]))\n",
    "        # elif name == \"nii\":\n",
    "        #     dataset = NII_with_label(data_path, FLAGS.data_extension, random_seed,\n",
    "        #           transform=transforms.Compose([transforms.ToTensor()]))\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized dataset %s' % name)\n",
    "        \n",
    "#         dataset_size = len(dataset)\n",
    "#         split = int(np.floor(test_split*dataset_size))\n",
    "#         train_, test_ = torch.utils.data.random_split(dataset, [dataset_size-split, split])\n",
    "#         train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size,\n",
    "#              shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "#         test_loader = torch.utils.data.DataLoader(test_, batch_size=batch_size,\n",
    "#              shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "#         return train_loader, test_loader\n",
    "\n",
    "        if datasplit_scheme==\"All\":\n",
    "            print(\"Using all data for trianing in data fetcher.\")\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                         shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "            return dataloader\n",
    "        elif datasplit_scheme == \"Test\":\n",
    "            print(\"Test mode in data fetcher.\")\n",
    "            dataset_size = len(dataset)\n",
    "            indices = list(range(dataset_size))\n",
    "            if shuffle:\n",
    "                np.random.seed(random_seed)\n",
    "                np.random.shuffle(indices)\n",
    "            fold_base = int(np.floor(dataset_size)/xfold)\n",
    "            fold_rec = [None] * xfold\n",
    "            for fold_gen in range(xfold-1):\n",
    "                list_tmp = list(np.arange(fold_gen*fold_base, (fold_gen+1)*fold_base, dtype=np.int32))\n",
    "                fold_rec[fold_gen] = list_tmp\n",
    "            fold_rec[xfold-1] = list(np.arange((xfold-1)*fold_base, dataset_size, dtype=np.int32))\n",
    "            \n",
    "            assert(fold_idx < xfold)\n",
    "            fold_train = list()\n",
    "            for fold_gen in range(xfold):\n",
    "                if fold_gen == fold_idx:\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_train = fold_train + fold_rec[fold_gen]\n",
    "            fold_test = fold_rec[fold_idx]\n",
    "            \n",
    "            indices = np.asarray(indices)\n",
    "            train_indices = list(indices[fold_train])\n",
    "            test_indices  = list(indices[fold_test])\n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            test_sampler  = SubsetRandomSampler(test_indices)\n",
    "\n",
    "            train_loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            test_loader   = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "            return train_loader, test_loader\n",
    "        elif datasplit_scheme == \"Valid\":\n",
    "            print(\"Validation mode in data fetcher.\")\n",
    "            dataset_size = len(dataset)\n",
    "            indices = list(range(dataset_size))\n",
    "            split = int(np.floor(test_split*dataset_size))\n",
    "            if shuffle:\n",
    "                np.random.seed(random_seed)\n",
    "                np.random.shuffle(indices)\n",
    "            test_indices = indices[dataset_size-split:]\n",
    "            train_valid_indices = indices[:dataset_size-split]\n",
    "            \n",
    "            fold_base = int(np.floor((dataset_size - split)/xfold))\n",
    "            fold_rec = [None]*xfold\n",
    "            for fold_gen in range(xfold-1):\n",
    "                list_tmp = list(np.arange(fold_gen*fold_base, (fold_gen+1)*fold_base, dtype=np.int32))\n",
    "                fold_rec[fold_gen] = list_tmp\n",
    "            fold_rec[xfold-1] = list(np.arange((xfold-1)*fold_base, dataset_size-split, dtype=np.int32))\n",
    "            \n",
    "            assert(fold_idx < xfold)\n",
    "            fold_train = list()\n",
    "            for fold_gen in range(xfold):\n",
    "                if fold_gen == fold_idx:\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_train = fold_train + fold_rec[fold_gen]\n",
    "            fold_valid = fold_rec[fold_idx]\n",
    "            \n",
    "            train_valid_indices = np.asarray(train_valid_indices)\n",
    "            train_indices = list(train_valid_indices[fold_train])\n",
    "            valid_indices = list(train_valid_indices[fold_valid])\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "            test_sampler  = SubsetRandomSampler(test_indices)\n",
    "            train_loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            valid_loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "            test_loader   = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "            return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings    = return_settings()\n",
    "# parser      = Argparser(settings)\n",
    "# general     = return_general(0)\n",
    "# arch1       = return_arch(15)\n",
    "# arch2       = return_arch(16)\n",
    "# data_params = return_data_settings(0)\n",
    "\n",
    "# epochs           = data_params[\"epochs\"]\n",
    "# batch_size       = data_params[\"batch_size\"]\n",
    "# batch_workers    = data_params[\"batch_workers\"]\n",
    "# shuffle          = data_params[\"shuffle\"]\n",
    "# drop_last        = data_params[\"drop_last\"]\n",
    "# datasplit_scheme = data_params[\"datasplit_scheme\"]\n",
    "# test_split       = data_params[\"test_split\"]\n",
    "# xfold            = data_params[\"xfold\"]\n",
    "# fold_idx         = data_params[\"fold_idx\"]\n",
    "# random_seed      = data_params[\"random_seed\"]\n",
    "\n",
    "# train_loader1, test_loader1 = Data_fetcher.fetch_dataset_wValidation(FLAGS.dataset, FLAGS.data_path, batch_size, batch_workers, shuffle, drop_last, 0.5, datasplit_scheme, test_split, xfold, fold_idx, random_seed)\n",
    "\n",
    "# for i, data in enumerate(train_loader1):\n",
    "#     if i == 0:\n",
    "#         sample = data['vol'][0].squeeze(0)\n",
    "#         plt.imshow(sample[:, :, 103], cmap=\"gray\")\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a587393fa550700e0fba79d0bdedf45b42916a28255a920c0c28087d13312d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
